<chapter xml:id="Tuning"
	 xmlns:xlink="http://www.w3.org/1999/xlink">
  <title>Tuning</title>
  <section>
    <title>Introduction</title>
    <para>
      This chapter contains imformation useful to obtain the best performance
      of the LIXA state server (<command>lixad</command>) in your environment.
      As explained in <xref linkend="Configuring_the_server"/> the server
      provide some configuration parameters that can be tuned.
    </para>
    <para>
      There are basically two tuning paths:
      <itemizedlist mark='bullet'>
	<listitem><para>
	  number of server threads: the level of internal parallelism can be
	  tuned according with the number of available CPUs
	</para></listitem>
	<listitem><para>
	  disk performance: several parameters are available to find the best
	  trade off between performance and reliability
	</para></listitem>
      </itemizedlist>
      The two tuning paths are typically not fully independent: increasing the
      number of server threads increases the pressure to the disk(s) and vice
      versa.
    </para>
    <section>
      <title>Number of server threads</title>
      <para>
	The LIXA state server is a multi-threaded process with one network
	listener and many <quote>managers</quote>; every manager runs
	in a dedicated thread. Choosing the optimal number of threads requires
	some trials: following the <quote>just work</quote> concept
	the default configuration specifies 3 threads, but your 
	installation might perform better using a different number (in the
	below paragraphs you can collect some hints).
      </para>
      <note><para>
	If you use the <emphasis>journal based</emphasis> state engine, 
	<command>lixad</command> will start 3 different POSIX threads for every
	server thread; if you use the <emphasis>traditional</emphasis> state
	engine, <command>lixad</command> will start a single POSIX thread for
	every server thread.
      </para></note>
      <para>
	Refer to <xref linkend="Configuring_the_server"/> for information about
	how to specify the number of server threads.
      </para>
    </section>
    <section>
      <title>Disk performance</title>
      <para>
	To get the best IO performance, <command>lixad</command> provides 3
	complementary strategies:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	    scattering IO through different disks: every server thread points
	    its own set of state files, if your system provides independent
	    disks, you can assign them to different server threads
	  </para></listitem>
	  <listitem><para>
	    choosing the state engine type: two state engines,
	    <emphasis>traditional</emphasis> and
	    <emphasis>journal based</emphasis>; the first has
	    a proven track of reliability because it has been deployed in
	    production environments since 2009 while the latter is a new
	    engine released in 2020 and provides amazingly low latency
	  </para></listitem>
	  <listitem><para>
	    configuring the state engine: depending on the previous choice,
	    different options are available to optimize the performance of
	    the state engine
	  </para></listitem>
	</itemizedlist>
      </para>
    </section>
  </section>
  <section>
    <title>Scattering IO through different disks</title>
    <para>
      As explained in <xref linkend="Configuring_the_server"/> every
      manager inside the LIXA state server uses a specific path for
      its status files. If you specify path associated to independent
      disks, you will obtain I/O independence for every the LIXA
      server thread.
    </para>
    <para>
      The path specified can be a directory name (it must terminate with a
      slash) or the prefix of a file name. Different server threads must
      specify different paths.
    </para>
  </section>
  <section>
    <title>Choosing the state engine type</title>
    <para>
      <emphasis>Journal based</emphasis> state engine has been designed to
      replace the <emphasis>traditional</emphasis> state engine and, on the
      long term, the latter might be removed from the state server. 
      From a performance point of view, there's no chance for the traditional
      engine to compete against the new journal one; nevertheless, due to the
      proven robustness, it's expected the usage of the traditional state
      engine will continue for some time, especially when high performance
      is not a must.
    </para>
    <para>
      Independently from the chosen state engine, some common concept requires
      understanding to operate an aware choice.
    </para>
    <para>
      First of all, the LIXA state server does not persist the data used by
      the Application Program, it only persists the state of the transactions.
      In the event that the state of a transaction is not preserved, you
      don't miss the data, but the state of the transaction that was
      manipulating the data.
      The direct consequence of the previous statement is that: in the worst
      case, if LIXA state server miss the state of the transaction, you have
      to rollback or commit the transaction manually. This is obviously not a
      situation you want to frequently have in your normal operations and this
      is why LIXA state server provides the highest level of resiliency.
    </para>
    <para>
      If you want to be 100% sure you don't miss the state of the transaction,
      you must be 100% sure that the state of the transaction has been
      recorded in a durable way, typically in some storage device.
      Writing storage, even the fastest storage, is still a quite slow
      operation in comparison with other communication and computing
      operations. 
      Very high performance storage can provide write latency below 1ms, but
      in a typical production environment, having 5ms write latency to the
      storage subsystem can be considered a good scenario.
      Incredibly enough, 5ms are a huge time for contemporary computing and
      contemporary networking.
    </para>
    <para>
      To make a long story short, this is a typical trade-off between speed
      and safety: the faster you go, the less safe you can be.
      In a real case scenario, 100% safety is unlikely to be the best choice:
      it's capped by the technology limits and it risks to unnecessarily slow
      down your whole system. 
    </para>
    <para>
      The good news is that the LIXA state engines, both journal and
      traditional, provide parameters to tune the behavior.
    </para>
    <section>
      <title>Tuning the journal state engine</title>
      <para>
	The journal state engine provides two levels of resilience with two
	different Recovery Point Objectives depending on the type of crash
	encountered by the LIXA state server.
      </para>
      <section>
	<title>Soft crash RPO</title>
	<para>
	  A <emphasis>soft crash</emphasis> is a crash that happens to the
	  LIXA state server process (<command>lixad</command>), but it does not
	  break the operating system. Common examples are:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      killing <command>lixad</command>
	    </para></listitem>
	    <listitem><para>
	      <command>lixad</command> crashes due to some unexpected reason
	    </para></listitem>
	  </itemizedlist>
	  In the event of a <emphasis>soft crash</emphasis>, the restart
	  usually happens from the last active state table file without any
	  data miss. As a consequence, when restart from the last active
	  state table succeeds, the RPO is zero and there's no transaction
	  state missing.
	</para>
      </section>
      <section>
	<title>Hard crash RPO</title>
	<para>
	  A <emphasis>hard crash</emphasis> is a crash related to the
	  operating system or a damage to state table files. Common examples
	  are:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      operating system / hypervisor crashes
	    </para></listitem>
	    <listitem><para>
	      hardware failure
	    </para></listitem>
	    <listitem><para>
	      disk content corruption
	    </para></listitem>
	  </itemizedlist>
	  In the event of a <emphasis>hard crash</emphasis>, the restart
	  happens from the last consistent active state table file plus all
	  the available consistent state log records.
	</para>
	<para>
	  Due to different strategies of log flushing, the corresponding RPO
	  can be greater than zero.
	</para>
      </section>
      <section>
	<title>Impact of parameter <parameter>min_elapsed_sync_time</parameter></title>
	<para>
	  The parameter fixes the minimum delay between the need of a
	  transaction state flushing and the start of the I/O operation:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      a low value slows down the <command>lixad</command> server
	      due to frequent asking for log flushing
	    </para></listitem>
	    <listitem><para>
	      a value greater than zero increases the RPO for 
	      <emphasis>hard crash</emphasis> by the same amount
	    </para></listitem>
	    <listitem><para>
	      to have a guaranteed RPO=0 in the event of
	      <emphasis>hard crash</emphasis>, this parameter must be set to
	      zero
	    </para></listitem>
	  </itemizedlist>
	</para>
      </section>
      <section>
	<title>Impact of parameter <parameter>max_elapsed_sync_time</parameter></title>
	<para>
	  The parameter fixes the maximum delay between the need of a
	  transaction state flushing and the start of the I/O operation:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      a low value slows down the <command>lixad</command> server
	      due to frequent asking for log flushing
	    </para></listitem>
	    <listitem><para>
	      a value greater than zero increases the RPO for 
	      <emphasis>hard crash</emphasis> by the same amount
	    </para></listitem>
	    <listitem><para>
	      to have a guaranteed RPO=0 in the event of
	      <emphasis>hard crash</emphasis>, this parameter must be set to
	      zero
	    </para></listitem>
	  </itemizedlist>
	  The parameter must be greater or equal to
	  <parameter>min_elapsed_sync_time</parameter>: if both are set to
	  value zero, state log file is flushed as soon as a transaction needs
	  to persist a new state.
	</para>
      </section>
      <section>
	<title>Impact of parameter <parameter>log_size</parameter></title>
	<para>
	  The parameter specifies the desired amount of disk space that must
	  be used of every state log file. A state log file can be switched
	  only when the previous state table synchronization has been
	  completed. In presence of slow disks, a larger value can be helpful
	  to obtain better performances.
	</para>
	<para>
	  The parameter has no direct impact on RPO.
	</para>
      </section>
      <section>
	<title>Impact of parameter <parameter>max_buffer_log_size</parameter></title>
	<para>
	  The parameter specifies the quantity of RAM used as buffer for
	  log writing: under some circumstances, higher values can improve
	  performances.
	</para>
	<para>
	  The parameter has no direct impact on RPO.
	</para>
      </section>
      <section>
	<title>Impact of parameters <parameter>log_o_*</parameter></title>
	<para>
	  The parameters specify the corresponding flags that must be used for
	  state log files; as a general rule, 
	  <parameter>log_o_direct="1"</parameter> (O_DIRECT) is faster than
	  <parameter>log_o_dsync="1"</parameter> (O_DSYNC) and
	  <parameter>log_o_dsync="1"</parameter> (O_DSYNC) is faster than
	  <parameter>log_o_sync="1"</parameter> (O_SYNC).
	</para>
	<important><para>
	  As a general rule, only <parameter>log_o_sync=1</parameter> in
	  association with 
	  <parameter>min_elapsed_sync_time="0"</parameter> and
	  <parameter>max_elapsed_sync_time="0"</parameter> provides RPO=0
	  in the event of <emphasis>hard crash</emphasis>.
	</para></important>
	<para>
	  In real life scenarios, depending on the characteristics of the
	  storage subsystem, less restrictive options can be used.
	  The best configuration requires further investigation about the
	  specific hardware configuration.
	</para>
	<note><para>
	  Parameters <parameter>log_o_*</parameter> can be uses together, for
	  example you can specify both <parameter>log_o_direct="1"</parameter> 
	  and <parameter>log_o_dsync="1"</parameter> to combine the effects of
	  O_DIRECT and O_DSYNC flags for log I/O.
	</para></note>
      </section>
    </section>
    <section>
      <title>Tuning the traditional state engine</title>
      <para>
	The traditional state engine provides a single level of resilience
	and no difference among types of crash encountered by the LIXA state
	server.
      </para>
    <para>
      Only two parameters can be configured, the others are ignored:
      <parameter>min_elapsed_sync_time</parameter> and 
      <parameter>max_elapsed_sync_time</parameter>. 
    </para>
    <section>
      <title>Impact of parameter <parameter>min_elapsed_sync_time</parameter></title>
	<para>
	  The parameter fixes the minimum delay between the need of a
	  transaction state flushing and the start of the memory map sync
	  operation:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      a low value slows down the <command>lixad</command> server
	      due to frequent asking for map syncing
	    </para></listitem>
	    <listitem><para>
	      a value greater than zero increases the RPO by the same amount
	    </para></listitem>
	    <listitem><para>
	      to have a guaranteed RPO=0 this parameter must be set to
	      zero
	    </para></listitem>
	  </itemizedlist>
	</para>
      </section>
      <section>
	<title>Impact of parameter <parameter>max_elapsed_sync_time</parameter></title>
	<para>
	  The parameter fixes the maximum delay between the need of a
	  transaction state flushing and the start of the memory map sync:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
	      a low value slows down the <command>lixad</command> server
	      due to frequent asking for map syncing
	    </para></listitem>
	    <listitem><para>
	      a value greater than zero increases the RPO by the same amount
	    </para></listitem>
	    <listitem><para>
	      to have a guaranteed RPO=0 this parameter must be set to
	      zero
	    </para></listitem>
	  </itemizedlist>
	  The parameter must be greater or equal to
	  <parameter>min_elapsed_sync_time</parameter>: if both are set to
	  value zero, memory map is synchronized as soon as a transaction needs
	  to persist a new state.
	</para>
      </section>
    </section>
    <section>
      <title>Balancing performance and resilience</title>
      <important>
	<para>
	  The higher the value of RPO, the higher the chance
	  you will have to perform manual recovery in the case of a server
	  crash (manual recovery is explained in 
	  <xref linkend="Manual_cold_recovery"/>).
	</para>
	<para>
	  On the other hand, don't force RPO=0 if you don't have clear evidence
	  that you need it: depending on your business requirements and your 
	  hardware configuration, especially
	  if you use the <emphasis>journal based state engine</emphasis>, the
	  need for RPO=0 might be not necessary: as you will see in the next
	  section, the performance benefit could be huge.
	</para>
      </important>
    </section>
  </section>
  <section xml:id="Tuning_example">
    <title>A tuning example</title>
    <para>
      This section explains a tuning example you can use as a starting point
      to develop your own tuning strategy.
    </para>
    <para>
      The utility program <command>lixat</command>, introduced in
      <xref linkend="Starting_test_utility"/> can be used as a benchmark tool
      specifying <parameter>-b</parameter> 
      (<parameter>--benchmark</parameter>) option.
      The available command options can be retrieved with
      <parameter>--help</parameter>:
      <screen>
tiian@ubuntu:~$ /opt/lixa/bin/lixat --help
Usage:
  lixat [OPTION...] - LIXA test utility

Help Options:
  -?, --help                  Show help options

Application Options:
  -c, --commit                Perform a commit transaction
  -r, --rollback              Perform a rollback transaction
  -v, --version               Print package info and exit
  -b, --benchmark             Perform benchmark execution
  -o, --open-close            Execute tx_open &amp; tx_close for every transaction [benchmark only]
  -s, --csv                   Send result to stdout using CSV format [benchmark only]
  -l, --clients               Number of clients (threads) will stress the state server [benchmark only]
  -d, --medium-delay          Medium (random) delay between TX functions [benchmark only]
  -D, --delta-delay           Delta (random) delay between TX functions [benchmark only]
  -p, --medium-processing     Medium (random) delay introduced by Resource Managers operations between tx_begin and tx_commit/tx_rollback [benchmark only]
  -P, --delta-processing      Delta (random) delay introduced by Resource Managers operations between tx_begin and tx_commit/tx_rollback [benchmark only]
      </screen>
      These are the interesting options in benchmark mode:
      <itemizedlist mark='bullet'>
	<listitem><para>
	    commit transactions (<parameter>-c</parameter> or
	    <parameter>--commit</parameter>)
	</para></listitem>
	<listitem><para>
	    rollback transactions (<parameter>-r</parameter> or
	    <parameter>--rollback</parameter>)
	</para></listitem>
	<listitem><para>
	    one couple of <function>tx_open()/tx_close()</function> for every
	    transaction (<parameter>-o</parameter> or
	    <parameter>--open-close</parameter>); alternatively only one
	    couple of <function>tx_open()/tx_close()</function> will be used
	    for all the transactions (<function>tx_open()/tx_begin()/tx_commit()/tx_begin()/tx_commit()/.../tx_close()</function>)
	</para></listitem>
	<listitem><para>
	    number of clients connected to the LIXA state server
	    (<parameter>-l</parameter> or <parameter>--clients</parameter>)
	</para></listitem>
	<listitem><para>
	    delay introduced by Application Program logic between 
	    <function>tx_*</function> functions (
	    <parameter>-d, --medium-delay, -D, --delta-delay</parameter>)
	</para></listitem>
	<listitem><para>
	    delay introduced by Resource Managers logic between
	    <function>tx_begin</function> and <function>tx_commit</function>
	    (or <function>tx_rollback</function>) functions (
	    <parameter>-p, --medium-processing, -P, --delta-processing</parameter>)
	</para></listitem>
      </itemizedlist>
    </para>
    <section>
      <title><command>lixat</command> benchmark behavior</title>
      <para>
	This is a sketch of <command>lixat</command> algorithm when
	<parameter>-o</parameter> or <parameter>--open-close</parameter>
	option is specified:
	<screen>
loop (1..100)
    sleep(random[d-D/2, d+D/2])
    tx_open()
    sleep(random[d-D/2, d+D/2])
    tx_begin()
    sleep(random[p-P/2, p+P/2])
    tx_commit()
    sleep(random[d-D/2, d+D/2])
    tx_close()
    sleep(random[d-D/2, d+D/2])
end loop
	</screen>
	With default delays the sleeping pauses are the following:
	<screen>
sleep(random[d-D/2, d+D/2]) --&gt; [500, 1500] microseconds
sleep(random[p-P/2, p+P/2]) --&gt;  [50, 150]  milliseconds	  
	</screen>
        without <parameter>-o</parameter> or 
	<parameter>--open-close</parameter> option <command>lixat</command>
	does not
	call <function>tx_open()/tx_close()</function> for every cycle and
	the algorithm becomes the following one:
	<screen>
tx_open()
loop (1..100)
    sleep(random[d-D/2, d+D/2])
    sleep(random[d-D/2, d+D/2])
    tx_begin()
    sleep(random[p-P/2, p+P/2])
    tx_commit()
    sleep(random[d-D/2, d+D/2])
    sleep(random[d-D/2, d+D/2])
end loop
tx_close()
	</screen>
      </para>
      <para>
	Using <parameter>--open-close</parameter> parameter you will simulate 
	an Application Program that creates and destroy the transactional
	environment for every transaction.
      </para>
      <para>
	Omitting <parameter>--open-close</parameter> parameter you will 
	simulate an Application Program that reuses the transactional
	environment for 100 transactions.
      </para>
      <para>
	Your Application Program could live in the middle, 
	<command>lixat</command> can help you to figure out two different 
	theoretical scenarios.
      </para>
      <para>
	A shell command you can use to measure the performance of LIXA for
	10, 20, 30, ... 100 clients is the following one:
	<screen>
for l in 10 20 30 40 50 60 70 80 90 100 ; do /opt/lixa/bin/lixat -b -s -l $l ; done | grep -v '^ ' > /tmp/bench_result.csv
	</screen>
      </para>
    </section>
    <section>
      <title>Tuning example hardware characteristics</title>
      <para>
	This is the output of <filename>/proc/cpuinfo</filename> executed
	in the system hosting <command>lixad</command> state server:
	<screen>
	  processor: 0
	  vendor_id: GenuineIntel
	  cpu family: 15
	  model: 2
	  model name: Intel(R) Celeron(R) CPU 2.40GHz
	  stepping: 9
	  cpu MHz: 2405.521
	  cache size: 128 KB
	  fdiv_bug: no
	  hlt_bug: no
	  f00f_bug: no
	  coma_bug: no
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 2
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe up pebs bts cid xtpr
	  bogomips: 4811.04
	  clflush size: 64
	</screen>
	This is the output of <filename>/proc/cpuinfo</filename> executed
	in the system hosting <command>lixat</command> benchmark process:
	<screen>
	  processor: 0
	  vendor_id: GenuineIntel
	  cpu family: 6
	  model: 23
	  model name: Genuine Intel(R) CPU           U7300  @ 1.30GHz
	  stepping: 10
	  cpu MHz: 800.000
	  cache size: 3072 KB
	  physical id: 0
	  siblings: 2
	  core id: 0
	  cpu cores: 2
	  apicid: 0
	  initial apicid: 0
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 13
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
	  bogomips: 2593.73
	  clflush size: 64
	  cache_alignment: 64
	  address sizes: 36 bits physical, 48 bits virtual
	  power management:

	  processor: 1
	  vendor_id: GenuineIntel
	  cpu family: 6
	  model: 23
	  model name: Genuine Intel(R) CPU           U7300  @ 1.30GHz
	  stepping: 10
	  cpu MHz: 800.000
	  cache size: 3072 KB
	  physical id: 0
	  siblings: 2
	  core id: 1
	  cpu cores: 2
	  apicid: 1
	  initial apicid: 1
	  fpu: yes
	  fpu_exception: yes
	  cpuid level: 13
	  wp: yes
	  flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx lm constant_tsc arch_perfmon pebs bts rep_good aperfmperf pni dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm sse4_1 xsave lahf_lm tpr_shadow vnmi flexpriority
	  bogomips: 2593.50
	  clflush size: 64
	  cache_alignment: 64
	  address sizes: 36 bits physical, 48 bits virtual
	  power management:
	</screen>
      </para>
      <para>
	From the above specs you can guess these are not powerful 
	<quote>server systems</quote>. The systems are connected with a
	100 Mbit/s connection with an average latency of 95 microseconds:
	<screen>
--- 192.168.10.2 ping statistics ---
50 packets transmitted, 50 received, 0% packet loss, time 48997ms
rtt min/avg/max/mdev = 0.133/0.190/0.226/0.024 ms
	</screen>
      </para>
    </section>
    <section>
      <title>Results obtained with <parameter>--open-close</parameter> parameter</title>
      <note>
	<para>
	  All the tests saturated the CPU of the host executing 
	  <command>lixad</command> state server for the higher values of
	  connected clients.
	</para>
      </note>
      <para>
	The first picture shows the elapsed time associated to
	<function>tx_open()</function> increases quite linearly with the 
	number of connected clients. A <command>lixad</command> state server 
	configured with 3 managers
	(threads), <parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter> exploits the best
	scalability (purple line); a <command>lixad</command> state server 
	configured with 3 managers,
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter> shows a scalability
	very near to the best (light green line).
	<emphasis>The second one is a more robust configuration and should
	  be preferred</emphasis>.
      </para>
      <figure xml:id="tuning_01">
	<title>Elapsed time of tx_open() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_01.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	A completely different behavior is shown by
	<function>tx_begin(), tx_commit(), tx_close()</function> functions:
	the best scalability is obtained with 1 manager (thread), 
	<parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter>
	(yellow line); a quite optimal
	performance can be obtained with 1 manager (thread), 
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter> (orange line). 
	The second 
	configuration should be preferred because it's more robust than the
	first one. Using more threads does not give any benefit for these
	three functions.
      </para>
      <figure xml:id="tuning_02">
	<title>Elapsed time of tx_begin() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_02.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_03">
	<title>Elapsed time of tx_commit() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_03.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_04">
	<title>Elapsed time of tx_close() when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_04.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	In the last chart 
	you may note the aggregated values for all the transactions
	(100 transactions, the elapsed time is now expressed in seconds):
	the purple line (best configuration for <function>tx_open()</function>)
	is the best overall configuration. This time too, there are many
	performance equivalent configurations:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	      3 managers (thread), 
	      <parameter>min_elapsed_sync_time=20</parameter> and
	      <parameter>max_elapsed_sync_time=100</parameter>
	      (purple line)
	  </para></listitem>
	  <listitem><para>
	      3 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (light green line)
	  </para></listitem>
	  <listitem><para>
	      2 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (red line)
	  </para></listitem>
	  <listitem><para>
	      1 managers (thread), 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>	      
	      (orange line)
	  </para></listitem>
	</itemizedlist>
	The second configuration of the above list (light green) could be 
	considered the
	best from an overall performance point of view and from a safety
	point of view: the minimum elapsed synchronization time is 
	10 milliseconds.
      </para>
      <figure xml:id="tuning_05">
	<title>Overall elapsed time when the Application Program 
	  uses a couple of tx_open()/tx_close() for every 
	  couple of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_05.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
    </section>
    <section>
      <title>Results obtained without <parameter>--open-close</parameter> parameter</title>
      <note>
	<para>
	  All the tests saturated the CPU of the host executing 
	  <command>lixad</command> state server for the higher values of
	  connected clients.
	</para>
      </note>
      <para>
	Avoiding a lot of <function>tx_open()/tx_close()</function> the
	behavior of the system is quite different. It's interesting to note
	the system has two distinct modes:
	<itemizedlist mark='bullet'>
	  <listitem><para>
	      in the range [10,50] clients the scalability is quite linear 
	      if you adopt a super safe configuration with 
	      <parameter>min_elapsed_sync_time=0</parameter> and
	      <parameter>max_elapsed_sync_time=0</parameter>
	  </para></listitem>
	  <listitem><para>
	      in the range [10,50] clients the scalability is
	      <quote>superlinear</quote>
	      <footnote>
		<para>
		  <function>tx_begin(), tx_commit(), tx_close()</function>
		  response times change very few when the number of clients
		  rises
		</para>
	      </footnote>
	      if you adopt an asynchronous
	      conifiguration with 
	      <parameter>min_elapsed_sync_time=10</parameter> and
	      <parameter>max_elapsed_sync_time=50</parameter>
	      or higher values
	  </para></listitem>
	  <listitem><para>
	      in the range [60,100] clients the system tends to saturate
	      and the <quote>superlinear</quote> characteristic is vanishing;
	      neverthless, asynchronous configurations exploit lower
	      response time than synchronous ones
	  </para></listitem>
	</itemizedlist>
      </para>
      <figure xml:id="tuning_06">
	<title>Elapsed time of tx_open() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_06.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_07">
	<title>Elapsed time of tx_begin() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_07.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_08">
	<title>Elapsed time of tx_commit() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_08.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <figure xml:id="tuning_09">
	<title>Elapsed time of tx_close() when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_09.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <para>
	The last chart shows the average elapsed time spent for 
	<function>tx_*</function> functions by 100 transactions.
	The best performace is obtained with the less safe configuration:
	<parameter>min_elapsed_sync_time=20</parameter> and
	<parameter>max_elapsed_sync_time=100</parameter>
	(purple, cyan and yellow lines).
	The intermediate performance is obtained with the intermediate
	configuration:
	<parameter>min_elapsed_sync_time=10</parameter> and
	<parameter>max_elapsed_sync_time=50</parameter>
	(dark yellow, red and orange lines).
	The worst performance is obtained with the safest configuration:
	<parameter>min_elapsed_sync_time=0</parameter> and
	<parameter>max_elapsed_sync_time=0</parameter>
	(green, light green and blue lines).
      </para>
      <figure xml:id="tuning_10">
	<title>Overall elapsed time when the Application Program 
	  uses a couple of tx_open()/tx_close() for a 
	  batch of tx_begin()/tx_commit()</title>
	<mediaobject>
	  <imageobject>
	    <imagedata fileref="../images/LIXA_Tuning_10.png"/>
	  </imageobject>
	</mediaobject>
      </figure>
      <section>
	<title>Conclusions</title>
	<para>
	  The LIXA project gives you some parameters you can change to tune
	  your installation and get the best performance.
	  There is not a magic recipe you can adopt, but there are some
	  rules of thumb:
	  <itemizedlist mark='bullet'>
	    <listitem><para>
		if possible, avoid the usage of 
		<function>tx_open()/tx_close()</function> for every 
		transaction: if your business logic could batch more than
		one transaction inside the same session, your overall
		response time would be lower
	    </para></listitem>
	    <listitem><para>
		delayed disk synchronization will help you in obtaining
		better performance with the same hardware, but introducing
		too high delays will not give you extra performance
	    </para></listitem>
	    <listitem><para>
		delay disk synchronization introduce the risk to perform
		manual recovery as explained in
		<xref linkend="Delayed_synchronization_effects"/>.
	    </para></listitem>
	  </itemizedlist>
	  Write your own test program, with real Resource Managers,
	  and measure it: with a test environment you would be able to
	  fine tune your own installation.
	</para>
      </section>
    </section>
  </section>
</chapter>
