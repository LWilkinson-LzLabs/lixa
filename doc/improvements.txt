This file contains links to technologies and ideas that could improve LIXA.
It's just a place to avoid human memory leak.



*** Low Latency Persistence ***
A partial improvement, without completely redesigning the foundation of the
state server, can be obtained as explained in document "Low Latency State Server
Enhancement".
Useful info related to disk sync latency are available at these URLs:
http://yoshinorimatsunobu.blogspot.it/2014/03/why-buffered-writes-are-sometimes.html
http://yoshinorimatsunobu.blogspot.it/2009/05/overwriting-is-much-faster-than_28.html
http://www.thesubodh.com/2013/07/what-are-exactly-odirect-osync-flags.html
Another possible improvement of the current state server is the adoption of
files dedicated to the redo log: it must be understood if memory mapping or
standard write/fdatasync is the best solution for few records between two
syncpoints.

Stuff for redo log:
- pre-allocate it to avoid appending, reset the full file with 0 before start
  using it: the 4KB write aligned will create holes and during reading only a
  full 4KB of 0s would tell "no more records"
- use 4KB multiple to avoid the read-modify-write: this is easy because the
  record to flush are already available in RAM, just use a pre-defined static
  buffer and flush the piece that's interesting
- just in case, put an hash in front of every record: during redo log apply it
  would be very useful to avoid a "dirty last record" 
- all the record have an ID, 32 bit unsigned; a function _is_after() is 
  necessary to establish if an ID follows another (the ID cycle after 2^32
  records)
- option for O_DIRECT (to be tuned with utility, maybe lixau or lixat)
- option for one of O_SYNC or O_DSYNC (to be tuned with utility, O_SYNC default)
  even nothing can be acceptable for LIXA state server, but with the risk of
  missing some transactions
- source of truth is: last consistent state file + all the following log
  records
- consistent state file contains the first ID of log record to be applied
- when a state file must be enlarged, a new redo log is created
- when a switch to another state file must be performed, the "new" state file
  must be truncated to the size of the previous one, the data must be copied
  (it's an in memory copy), a new redo log is used, synchronization of the old
  state file is started in background with another dedicated thread
- redo log records are kept in memory until the buffer is full or a
  synchronization is asked
- redo log records buffer is 4K multiplier, configurable, allocated on heap
- pwrite is used to copy the buffer from memory to disk
- the tail of the redo log contains 0s, some unused space, but it doesn't
  matter
- every record in the redo log contains an hash to check consistency, the ID
  to understand the sequence and the data; the size of the record only if
  records are of different size (I don't think so)
- log record ID = 0 is reserved because unused log space is 0 filled
- change of state file happens when redo log reach a threashold when state file
  requires a resize (see above)
- 




2018-06-18 The best option I have found till now is based on snapshots +
journals. 3 snapshots (with msync like now) + 3 journals (with fdatasync) and
a background thread used to avoid pauses during snapshotting (snapshots must be
captured by the "just closed" state file, integrity is guaranteed by the
journals).
2018-12-13 A completely different approach would be the usage of a key/value
embeddable database like
leveldb
  https://github.com/google/leveldb
  http://www.lmdb.tech/bench/microbench/benchmark.html
or 
rocksdb
  https://rocksdb.org/
  http://rocksdb.blogspot.com/2013/11/the-history-of-rocksdb.html
even if a general purpose key/value could provide additional values and benefits
two points must be considered:
- from the published papers, they are not optimized for sync writes: the 
  impressive write performance they can reach requires async writes
- LIXA state server would require a major review because the current 
  implementation use a memory mapped file to obtain full in memory (read)
  access; accessing the data is just a matter of pointer dereferencing.
In the end, the key/value db is an idea, but only additional requirements will
justify such a change.



*** High Availability ***
After a lot of thinking about it, the best pattern aligned with LIXA
architecture is a client based high availability: a client connect to many
state servers in parallel and by mean of poll send and receive messages using
concurrent TCP/IP streams. The total latency becomes the latency introduced by
the slowest state server.
Client logic must guarantee that the minimum number of requested state server
reports exact the same answer. The state server set and the state server quorum
are configured by the user.
Some esamples:
- 2/2: two state servers, both must be aligned
- 3/2: three state servers, at least two must be aligned
- 5/3: five state servers, at least three must be aligned
Some necessary pre-requisites:
- state servers must have a unique id, used by the client to check them
- state server set must be registered by all the state servers and when 
  necessary used by the client to decide
Architecture plus:
- LIXA can report to application program a failure related to high availability
  when it happens, not when it's to late to take the best decision from the
  application perspective
Architecture minus:
- LIXA client and servers must all stay in a low latency network; but even in
  the case of a server/server propagation, the servers can't use an high
  latency network
- it genetates "a lot of" client/server network traffic (but it does not
  generate server/server traffic), so basically no a big issue inside the same
  datacenter



*** Monitoring & Metering ***
Prometheus is becoming more and more popular.
Creating a /meter url published by a library like 
https://www.libhttp.org/
could be effective, practical, easy and very nice to monitor with Grafana.



*** Serialization: ***

protobuf / protobuf-c
https://github.com/google/protobuf
https://github.com/protobuf-c/protobuf-c

Cap'n Proto
https://capnproto.org/index.html



*** Networking: ***

ZeroMQ
http://zeromq.org/

The C10K problem
http://www.kegel.com/c10k.html
