This file contains links to technologies and ideas that could improve LIXA.
It's just a place to avoid human memory leak.



*** XTA timeout ***
Naive analysis suggests some sort of timeout for xta_transaction_commit in
the event of a multiple applications configuration.
After some more thinking on the subject, I have found out two possible
"error cases":
1. a bug inside an Application Program that's cooperating in the DTP
2. a very slow commit phase in charge of some Resource Manager and/or the
   LIXA state server
Trying to solve an application bug with an infrastructure timeout does not
sound a good idea: fix the Application Program and you will be done.
The second "error case" seems to be more interesting, but it creates more
questions than answers: in a multiple application DTP there's high chance at
least one AP performed xa_prepare in the right way and interrupting the two
phase commit can generate further complexity that must be handled by a
recovery phase.
At the time of this writing (2018-04-17) there are a lot of unclear items:
1. what's type of hang will real application meet in production?
2. in what hang behaviors a timeout can be fruitful?
3. what should be the consequence of the timeout? (A signal to crash the AP,
   an exit(X>0) to close the AP, a LIXA return code after complete clean-up of
   most XTA resources, ...) Unfortunately there's no an "easy way" to handle
   it because LIXA supports and probably will support two completely different
   programming models: XTA and TX.
Nevertheless, here are some notes I collected in TODO and moved in this place
to avoid the trash can, but even to avoid the implementation of a sloppy
feature.
Implement timeout to avoid a too long block time for clients (timeout parameter
in xa_prepare step 8 and step 24 messages):
- client side only (remove from messsages)
- pick-up current time at function entry
- use poll to avoid too long blocking for msg put and msg get
- in the event of timeout, use shutdown to close the local half pipe of the
  socket
  - do all the necessary operation, maybe rollback resource managers if
    necessary
  - destroy all the objects
  - close the socket (it could be a not clean close if the server has not close
    the socket on its side in the meantime, but waiting will break the timeout)
  - connection can't be reused: invalidate Transaction Manager and Transaction
  - return to caller different return codes for the different timeout point
    (ended, prepared, committed)



*** Low Latency Persistence ***
A partial improvement, without completely redesigning the foundation of the
state server, can be obtained as explained in document "Low Latency State Server
Enhancement".
Useful info related to disk sync latency are available at these URLs:
http://yoshinorimatsunobu.blogspot.it/2014/03/why-buffered-writes-are-sometimes.html
http://yoshinorimatsunobu.blogspot.it/2009/05/overwriting-is-much-faster-than_28.html
Another possible improvement of the current state server is the adoption of
files dedicated to the redo log: it must be understood if memory mapping or
standard write/fdatasync is the best solution for few records between two
syncpoints.
2018-06-18 The best option I have found till now is based on snapshots +
journals. 3 snapshots (with msync like now) + 3 journals (with fdatasync) and
a background thread used to avoid pauses during snapshotting (snapshots must be
captured by the "just closed" state file, integrity is guaranteed by the
journals).
2018-12-13 A completely different approach would be the usage of a key/value
embeddable database like
leveldb
  https://github.com/google/leveldb
  http://www.lmdb.tech/bench/microbench/benchmark.html
or 
rocksdb
  https://rocksdb.org/
  http://rocksdb.blogspot.com/2013/11/the-history-of-rocksdb.html
even if a general purpose key/value could provide additional values and benefits
two points must be considered:
- from the published papers, they are not optimized for sync writes: the 
  impressive write performance they can reach requires async writes
- LIXA state server would require a major review because the current 
  implementation use a memory mapped file to obtain full in memory (read)
  access; accessing the data is just a matter of pointer dereferencing.
In the end, the key/value db is an idea, but only additional requires will
justify such a change.



*** High Availability ***
After a lot of thinking about it, the best pattern aligned with LIXA
architecture is a client based high availability: a client connect to many
state servers in parallel and by mean of poll send and receive messages using
concurrent TCP/IP streams. The total latency becomes the latency introduced by
the slowest state server.
Client logic must guarantee that the minimum number of requested state server
reports exact the same answer. The state server set and the state server quorum
are configured by the user.
Some esamples:
- 2/2: two state servers, both must be aligned
- 3/2: three state servers, at least two must be aligned
- 5/3: five state servers, at least three must be aligned
Some necessary pre-requisites:
- state servers must have a unique id, used by the client to check them
- state server set must be registered by all the state servers and when 
  necessary used by the client to decide
Architecture plus:
- LIXA can report to application program a failure related to high availability
  when it happens, not when it's to late to take the best decision from the
  application perspective
Architecture minus:
- LIXA client and servers must all stay in a low latency network; but even in
  the case of a server/server propagation, the servers can't use an high
  latency network
- it genetates "a lot of" client/server network traffic (but it does not
  generate server/server traffic), so basically no a big issue inside the same
  datacenter



*** Monitoring & Metering ***
Prometheus is becoming more and more popular.
Creating a /meter url published by a library like 
https://www.libhttp.org/
could be effective, practical, easy and very nice to monitor with Grafana.



*** Serialization: ***

protobuf / protobuf-c
https://github.com/google/protobuf
https://github.com/protobuf-c/protobuf-c

Cap'n Proto
https://capnproto.org/index.html



*** Networking: ***

ZeroMQ
http://zeromq.org/

The C10K problem
http://www.kegel.com/c10k.html
